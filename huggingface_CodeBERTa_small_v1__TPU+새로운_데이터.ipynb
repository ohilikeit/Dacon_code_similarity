{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohilikeit/Dacon_code_similarity/blob/main/huggingface_CodeBERTa_small_v1__TPU%2B%EC%83%88%EB%A1%9C%EC%9A%B4_%EB%8D%B0%EC%9D%B4%ED%84%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1olcCpkHEjA",
        "outputId": "bb69ce4c-9861-4aaa-f16c-2c5f936510f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkXaT59FHHmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a55e11c-e0e8-45ef-cc08-a7dc148f4a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code_similarity\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/code_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets\n",
        "! pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
        "import os\n",
        "import random\n",
        "assert os.environ['COLAB_TPU_ADDR']\n",
        "import transformers\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from transformers.utils.dummy_pt_objects import get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback, RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers.utils import logging\n",
        "\n",
        "dev = xm.xla_device()\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "transformers.set_seed(seed)\n",
        "transformers.enable_full_determinism(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "ZZ8YobNTL8X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-_bQ9WqEHc"
      },
      "outputs": [],
      "source": [
        "MODEL = \"huggingface/CodeBERTa-small-v1\"\n",
        "MAX_LEN = 512\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
        "dataset = load_dataset('csv', data_files={'train': 'train_data', 'test': 'val_data'}) # 읽어오려면 .csv 형태면 안된다. 저장할때 빼고 저장할 것 바이너리 파일. \n",
        "\n",
        "logging.set_verbosity_error() # 코드 실행시 script가 길어져서 생기는 오류를 방지하기 위한 코드 \n",
        "\n",
        "def example_fn(examples):\n",
        "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=MAX_LEN,truncation=True)\n",
        "    if 'similar' in examples:\n",
        "        outputs[\"labels\"] = examples[\"similar\"]\n",
        "    return outputs\n",
        "\n",
        "##### 둘 중 하나만 #####\n",
        "#dataset = dataset.map(example_fn, remove_columns=['code1', 'code2', 'similar'])\n",
        "#dataset.save_to_disk('dataset') # 일단 저장.. \n",
        "dataset = datasets.load_from_disk(\"dataset\")\n",
        "\n",
        "_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "_metric = load_metric(\"glue\", \"sst2\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds, labels = p\n",
        "    output =  _metric.compute(references=labels, predictions=np.argmax(preds, axis=-1),)\n",
        "    return output\n",
        "\n",
        "logging.set_verbosity_info() # 진행바 확인을 위해 logging 재설정 \n",
        "\n",
        "#MODEL = '/content/drive/MyDrive/code_similarity/experiments_eval_loss/checkpoint-11000' # 34000 checkpoint 이후 오류가 생겨 다시 불러와서 재학습시켰다. \n",
        "model = RobertaForSequenceClassification.from_pretrained(MODEL).to(dev)\n",
        "args = TrainingArguments(\n",
        "    'experiments_eval_loss',\n",
        "    evaluation_strategy = 'steps',\n",
        "    save_steps = 2000,\n",
        "    eval_steps = 2000,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model='eval_loss', # loss or eval_loss로 해보자, 할꺼면 greater_is_better = False로, 낮아져야하는거니깐 + step을 5정도로 늘리자 \n",
        "    greater_is_better = False,\n",
        "    per_device_train_batch_size=32, # 가능한 최대 크기 배치 사이즈임 \n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    warmup_steps=1000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay= 0.1,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    save_strategy='steps',\n",
        "    logging_strategy=\"steps\",\n",
        "    tpu_num_cores=8,                 # 8코어를 사용하고 있는지는 잘 모르겠다. \n",
        "    #gradient_accumulation_steps = 4, # gpu 메모리 줄이는법, 배치 사이즈랑 상충관계. 1 * 64 = 4 * 16 \n",
        "    #gradient_checkpointing=True, # forward 시에 활성값들 까먹고 backward에 재계산, 메모리 효율적, 시간은 더 걸림. \n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=_collator,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g42ojrRTlxyX",
        "outputId": "386a9bfe-34be-4c84-cdd8-99508c7ac5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3365893\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 210370\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16000' max='210370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 16000/210370 6:03:14 < 73:33:17, 0.73 it/s, Epoch 0/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.085100</td>\n",
              "      <td>0.057841</td>\n",
              "      <td>0.982237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.071300</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.983888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.067900</td>\n",
              "      <td>0.049420</td>\n",
              "      <td>0.981565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.057200</td>\n",
              "      <td>0.046761</td>\n",
              "      <td>0.985162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.052400</td>\n",
              "      <td>0.045875</td>\n",
              "      <td>0.985049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.057400</td>\n",
              "      <td>0.067207</td>\n",
              "      <td>0.978438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.054700</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.982084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.055600</td>\n",
              "      <td>0.052818</td>\n",
              "      <td>0.980567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-4000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-4000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-4000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-6000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-6000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-6000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-8000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-8000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-8000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-10000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-10000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-10000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-12000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-12000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-12000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-14000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-14000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-14000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-16000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-16000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-16000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from experiments_eval_loss/checkpoint-10000 (score: 0.04587457329034805).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16000, training_loss=0.06851361060142518, metrics={'train_runtime': 21795.5042, 'train_samples_per_second': 308.861, 'train_steps_per_second': 9.652, 'total_flos': 6.431391555297792e+16, 'train_loss': 0.06851361060142518, 'epoch': 0.15})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # 밑에 꺼 max_len만 512로 바꿨다. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8rrLY5aHGFI3",
        "outputId": "ced44158-4853-4180-f643-8daf0cc44b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3365893\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 210370\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='210370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4000/210370 4:13:54 < 218:26:03, 0.26 it/s, Epoch 0/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.170200</td>\n",
              "      <td>0.068907</td>\n",
              "      <td>0.977405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.095300</td>\n",
              "      <td>0.097291</td>\n",
              "      <td>0.965171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>0.072425</td>\n",
              "      <td>0.979431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.055360</td>\n",
              "      <td>0.983117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.051827</td>\n",
              "      <td>0.983942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.070200</td>\n",
              "      <td>0.060119</td>\n",
              "      <td>0.982351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.063600</td>\n",
              "      <td>0.059898</td>\n",
              "      <td>0.984145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.067800</td>\n",
              "      <td>0.063607</td>\n",
              "      <td>0.979480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-1000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-1000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-1500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-1500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-3000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-3000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-3000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-3500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-3500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-3500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-4000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-4000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-4000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from experiments_eval_loss/checkpoint-2500 (score: 0.05182696506381035).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4000, training_loss=0.0890111541748047, metrics={'train_runtime': 15234.8818, 'train_samples_per_second': 441.867, 'train_steps_per_second': 13.808, 'total_flos': 1.6098696724964352e+16, 'train_loss': 0.0890111541748047, 'epoch': 0.04})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-OQlFzSfMhl",
        "outputId": "b5a4a3ed-2196-4347-87cf-7d767adf3f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # 0.65 나옴 2시간 "
      ],
      "metadata": {
        "id": "8fXcSKanfYN-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97671d4e-35da-4641-c694-bd748c9b4903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3365893\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 210370\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18000' max='210370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 18000/210370 2:46:11 < 29:36:16, 1.80 it/s, Epoch 0/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.073349</td>\n",
              "      <td>0.976229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.074100</td>\n",
              "      <td>0.059417</td>\n",
              "      <td>0.980236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.068100</td>\n",
              "      <td>0.057774</td>\n",
              "      <td>0.980874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.057400</td>\n",
              "      <td>0.052624</td>\n",
              "      <td>0.982697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.055400</td>\n",
              "      <td>0.048165</td>\n",
              "      <td>0.981906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.055400</td>\n",
              "      <td>0.073156</td>\n",
              "      <td>0.977198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.054800</td>\n",
              "      <td>0.054029</td>\n",
              "      <td>0.983176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.052900</td>\n",
              "      <td>0.053371</td>\n",
              "      <td>0.984411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.055700</td>\n",
              "      <td>0.059199</td>\n",
              "      <td>0.981723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-4000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-4000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-4000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-6000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-6000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-6000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-8000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-8000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-8000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-10000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-10000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-10000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-12000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-12000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-12000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-14000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-14000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-14000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-16000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-16000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-16000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-18000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-18000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-18000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from experiments_eval_loss/checkpoint-10000 (score: 0.04816458001732826).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=18000, training_loss=0.07074858305189345, metrics={'train_runtime': 9972.6851, 'train_samples_per_second': 675.022, 'train_steps_per_second': 21.095, 'total_flos': 3.8150610812928e+16, 'train_loss': 0.07074858305189345, 'epoch': 0.17})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # 이건 loss 기준으로 새로 해본거 5시간 걸림. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h0gLbilNLSS-",
        "outputId": "2cdd5c9e-af42-4f80-a1ee-b97800635d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3365893\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 52594\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11099' max='52594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11099/52594 5:23:32 < 20:09:50, 0.57 it/s, Epoch 0.42/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.178000</td>\n",
              "      <td>0.067939</td>\n",
              "      <td>0.976506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.068700</td>\n",
              "      <td>0.065535</td>\n",
              "      <td>0.976219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.054400</td>\n",
              "      <td>0.058761</td>\n",
              "      <td>0.980098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>0.046534</td>\n",
              "      <td>0.981224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.043900</td>\n",
              "      <td>0.041628</td>\n",
              "      <td>0.984584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.039700</td>\n",
              "      <td>0.042472</td>\n",
              "      <td>0.984634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.036900</td>\n",
              "      <td>0.041390</td>\n",
              "      <td>0.985286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.037100</td>\n",
              "      <td>0.035270</td>\n",
              "      <td>0.986857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.035000</td>\n",
              "      <td>0.033718</td>\n",
              "      <td>0.987751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.034718</td>\n",
              "      <td>0.987238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.033900</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.986911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.032800</td>\n",
              "      <td>0.035968</td>\n",
              "      <td>0.987267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.031400</td>\n",
              "      <td>0.036748</td>\n",
              "      <td>0.987791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.033105</td>\n",
              "      <td>0.988142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.031009</td>\n",
              "      <td>0.988730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.029900</td>\n",
              "      <td>0.030456</td>\n",
              "      <td>0.988908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.029700</td>\n",
              "      <td>0.030049</td>\n",
              "      <td>0.988389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.033005</td>\n",
              "      <td>0.987485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.027800</td>\n",
              "      <td>0.030958</td>\n",
              "      <td>0.989076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.026900</td>\n",
              "      <td>0.035197</td>\n",
              "      <td>0.987440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.027600</td>\n",
              "      <td>0.035150</td>\n",
              "      <td>0.987440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>0.029455</td>\n",
              "      <td>0.989100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-1000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-1000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-1500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-1500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-2500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-2500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-2500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-3000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-3000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-3000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-3500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-3500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-3500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-4000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-4000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-4000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-4500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-4500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-4500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-5000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-5000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-5000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-5500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-5500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-5500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-6000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-6000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-6000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-6500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-6500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-6500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-7000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-7000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-7000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-7500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-7500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-7500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-8000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-8000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-8000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-8500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-8500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-8500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-9000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-9000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-9000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-9500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-9500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-9500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-10000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-10000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-10000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-10500\n",
            "Configuration saved in experiments_eval_loss/checkpoint-10500/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-10500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments_eval_loss/checkpoint-11000\n",
            "Configuration saved in experiments_eval_loss/checkpoint-11000/config.json\n",
            "Model weights saved in experiments_eval_loss/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments_eval_loss/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in experiments_eval_loss/checkpoint-11000/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         )\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "IpbGs0kEMjc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "568ecdd4-e439-4a96-ad6f-31bb95466c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3365893\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 52594\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35531' max='52594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35531/52594 10:27:52 < 5:01:32, 0.94 it/s, Epoch 1.35/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.051500</td>\n",
              "      <td>0.039323</td>\n",
              "      <td>0.985360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.036900</td>\n",
              "      <td>0.034815</td>\n",
              "      <td>0.986907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.031900</td>\n",
              "      <td>0.030895</td>\n",
              "      <td>0.988404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>0.032210</td>\n",
              "      <td>0.987578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.026600</td>\n",
              "      <td>0.034243</td>\n",
              "      <td>0.988922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.026700</td>\n",
              "      <td>0.032762</td>\n",
              "      <td>0.988838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.023700</td>\n",
              "      <td>0.030774</td>\n",
              "      <td>0.988754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.024100</td>\n",
              "      <td>0.025333</td>\n",
              "      <td>0.990133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.021900</td>\n",
              "      <td>0.028599</td>\n",
              "      <td>0.989392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.021100</td>\n",
              "      <td>0.027832</td>\n",
              "      <td>0.989347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.021900</td>\n",
              "      <td>0.026263</td>\n",
              "      <td>0.990331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.019100</td>\n",
              "      <td>0.028744</td>\n",
              "      <td>0.991032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.018900</td>\n",
              "      <td>0.022780</td>\n",
              "      <td>0.991650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.016900</td>\n",
              "      <td>0.026213</td>\n",
              "      <td>0.991437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.016900</td>\n",
              "      <td>0.024076</td>\n",
              "      <td>0.991126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.016400</td>\n",
              "      <td>0.023614</td>\n",
              "      <td>0.991734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>0.023732</td>\n",
              "      <td>0.991215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-2000\n",
            "Configuration saved in experiments/checkpoint-2000/config.json\n",
            "Model weights saved in experiments/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-4000\n",
            "Configuration saved in experiments/checkpoint-4000/config.json\n",
            "Model weights saved in experiments/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-4000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-6000\n",
            "Configuration saved in experiments/checkpoint-6000/config.json\n",
            "Model weights saved in experiments/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-6000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-8000\n",
            "Configuration saved in experiments/checkpoint-8000/config.json\n",
            "Model weights saved in experiments/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-8000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-10000\n",
            "Configuration saved in experiments/checkpoint-10000/config.json\n",
            "Model weights saved in experiments/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-10000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-12000\n",
            "Configuration saved in experiments/checkpoint-12000/config.json\n",
            "Model weights saved in experiments/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-12000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-14000\n",
            "Configuration saved in experiments/checkpoint-14000/config.json\n",
            "Model weights saved in experiments/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-14000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-16000\n",
            "Configuration saved in experiments/checkpoint-16000/config.json\n",
            "Model weights saved in experiments/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-16000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-18000\n",
            "Configuration saved in experiments/checkpoint-18000/config.json\n",
            "Model weights saved in experiments/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-18000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-20000\n",
            "Configuration saved in experiments/checkpoint-20000/config.json\n",
            "Model weights saved in experiments/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-20000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-22000\n",
            "Configuration saved in experiments/checkpoint-22000/config.json\n",
            "Model weights saved in experiments/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-22000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-24000\n",
            "Configuration saved in experiments/checkpoint-24000/config.json\n",
            "Model weights saved in experiments/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-24000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-26000\n",
            "Configuration saved in experiments/checkpoint-26000/config.json\n",
            "Model weights saved in experiments/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-26000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-28000\n",
            "Configuration saved in experiments/checkpoint-28000/config.json\n",
            "Model weights saved in experiments/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-28000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-30000\n",
            "Configuration saved in experiments/checkpoint-30000/config.json\n",
            "Model weights saved in experiments/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-30000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-32000\n",
            "Configuration saved in experiments/checkpoint-32000/config.json\n",
            "Model weights saved in experiments/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-32000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 202391\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to experiments/checkpoint-34000\n",
            "Configuration saved in experiments/checkpoint-34000/config.json\n",
            "Model weights saved in experiments/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in experiments/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in experiments/checkpoint-34000/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IVC9yB8qDVa",
        "outputId": "4e16b294-0ace-4840-9e47-0738c448a8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "\n",
        "logging.set_verbosity_error() \n",
        "TEST = \"sample_test\"\n",
        "SUB = \"sample_submission.csv\"\n",
        "\n",
        "#test_dataset = load_dataset(\"csv\", data_files=TEST)['train']\n",
        "#test_dataset = test_dataset.map(example_fn, remove_columns=['code1', 'code2'])\n",
        "\n",
        "#test_dataset.save_to_disk('test_dataset_v2')\n",
        "test_dataset = datasets.load_from_disk(\"test_dataset\")\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "df = pd.read_csv(SUB)\n",
        "df['similar'] = np.argmax(predictions.predictions, axis=-1)\n",
        "df.to_csv('prediction_tpu.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "hnfqUQyBF6F3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "f21b4e7e-7b26-4a05-e46b-13c0e2b276b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5616' max='5616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5616/5616 08:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        pair_id  similar\n",
              "0             1        1\n",
              "1             2        1\n",
              "2             3        0\n",
              "3             4        1\n",
              "4             5        1\n",
              "...         ...      ...\n",
              "179695   179696        1\n",
              "179696   179697        1\n",
              "179697   179698        1\n",
              "179698   179699        1\n",
              "179699   179700        1\n",
              "\n",
              "[179700 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ea5f8e5-2ef2-40d2-b3d6-9ed4b4926b44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>similar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179695</th>\n",
              "      <td>179696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179696</th>\n",
              "      <td>179697</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179697</th>\n",
              "      <td>179698</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179698</th>\n",
              "      <td>179699</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179699</th>\n",
              "      <td>179700</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179700 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ea5f8e5-2ef2-40d2-b3d6-9ed4b4926b44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ea5f8e5-2ef2-40d2-b3d6-9ed4b4926b44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ea5f8e5-2ef2-40d2-b3d6-9ed4b4926b44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "huggingface/CodeBERTa-small-v1 _TPU+새로운 데이터.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}